{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ce929a6-fb68-4089-9b76-fb5a7d7501f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version  2.10.0\n"
     ]
    }
   ],
   "source": [
    "# This script is an example of fine-tuning one of the \n",
    "# BlastoSPIM models on other data - in this case Organoids data\n",
    "\n",
    "# Cell 1: setting path to blastospim-processing-pipeline-Jupyter code and checking environment\n",
    "# NOTE: please change path_to_code to your own path below\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "import tifffile as tif\n",
    "from csbdeep.utils import Path, normalize\n",
    "from csbdeep.io import save_tiff_imagej_compatible\n",
    "\n",
    "from stardist import fill_label_holes, random_label_cmap, calculate_extents, gputools_available\n",
    "from stardist import Rays_GoldenSpiral\n",
    "from stardist.matching import matching, matching_dataset\n",
    "from stardist.models import Config3D, StarDist3D, StarDistData3D\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "print('tensorflow version ',tf.__version__)\n",
    "lbl_cmap = random_label_cmap()\n",
    "np.random.seed(42)\n",
    "\n",
    "# Specify path to blastospim-processing-pipeline-Jupyter directory\n",
    "# path_to_code = \"/path/to/your/blastospim-processing-pipeline-Jupyter/\"\n",
    "path_to_code = \"/Users/hnunley/Pictures/blastospim-processing-pipeline-Jupyter/\"\n",
    "\n",
    "## Augmentation temporarily commented out -- TODO: put back in\n",
    "#print(\"Augmentation\")\n",
    "#from pyimgaug3d.augmentation import GridWarp, Flip, Identity\n",
    "#from pyimgaug3d.augmenters import ImageSegmentationAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe07dabc-4766-4361-8d51-7ff2e78e5098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You do not have GPU access.\n",
      "Stardist flag for GPU also working\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: test whether you have GPU access\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "if tf.test.gpu_device_name() == '':\n",
    "    print('You do not have GPU access.')\n",
    "else:\n",
    "    print('You have GPU access')\n",
    "\n",
    "if gputools_available():\n",
    "    print(\"Stardist flag for GPU also working\")\n",
    "else:\n",
    "    print(\"Stardist flag for GPU NOT working, go back check out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbce29bb-441c-4719-8f65-a1f9096f0142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3: set these for training for training\n",
    "\n",
    "nephochs = 100 # number of epochs, use 200 or 400 for actual training\n",
    "nsteps_per_epoch = 100 # nsteps_per_epoch = 100  -- use 100 for actual training\n",
    "z_ptch_size = 32 # z_ptch_size = 32  -- z dimension of tha patch size, 32 x 256 x 256 gives best results\n",
    "ptch_size = 256 # ptch_size = 256 -- x and y dimension of the patch size, don't have to be same\n",
    "start_percentile = 1 # start_percentile = 1 -- start percentile for normalization of images\n",
    "end_percentile = 99.8 # end_percentile = 99.8 -- end percentile for normalization of images\n",
    "n_channel = 1 # n_channel = 1  # n_channel = 3 for RGB images and (2D 3 slice images) and 1 otherwise\n",
    "axis_norm = (0, 1, 2) # axis_norm = (0, 1, 2)  # (0, 1, 2) for normalizing channels independently does not matter if n_channel = 1\n",
    "num_val = 10000 # num_val = 10000 # for testing whether the script works set them to a small nuber such as 2 otherwise set it to 100000\n",
    "num_trn = 20000 # num_trn = 20000 # for testing whether the script works set them to a small nuber such as 2 otherwise set it to 100000\n",
    "\n",
    "# the actual validation and training examples are the min(num_val/num_trn, len(data_valX, data_trainX) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "673d9654-7338-4095-a642-35b18ed2e3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4: set up your training data\n",
    "\n",
    "# for best performance - this data should be anisotropic (~10,1,1) along (z,y,x) -- i.e. slices in z are ~10 x further apart than the units in x and y\n",
    "# data expected to be 3D and in tif or np format\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "#data_path = \"/path/to/your/training/and/validation/Data/\"\n",
    "data_path = path_to_code # this assumes you downloaded the relevant sample data into your path_to_code\n",
    "\n",
    "# store paths to all training data (here timepoints 000, 250, 350 in annotated data)\n",
    "# These data_... store paths to all raw and label images in the training set.\n",
    "data_x_000 = sorted(glob(data_path + \"2022_64x256x256_Platynereis/dataset_hdf5_000/dataset_hdf5_000/images/*.npy\"))\n",
    "data_y_000 = sorted(glob(data_path + \"2022_64x256x256_Platynereis/dataset_hdf5_000/dataset_hdf5_000/masks/*.npy\"))\n",
    "\n",
    "data_x_250 = sorted(glob(data_path + \"2022_64x256x256_Platynereis/dataset_hdf5_250/dataset_hdf5_250/images/*.npy\"))\n",
    "data_y_250 = sorted(glob(data_path + \"2022_64x256x256_Platynereis/dataset_hdf5_250/dataset_hdf5_250/masks/*.npy\"))\n",
    "\n",
    "data_x_350 = sorted(glob(data_path + \"2022_64x256x256_Platynereis/dataset_hdf5_350/dataset_hdf5_350/images/*.npy\"))\n",
    "data_y_350 = sorted(glob(data_path + \"2022_64x256x256_Platynereis/dataset_hdf5_350/dataset_hdf5_350/masks/*.npy\"))\n",
    "\n",
    "data_x = data_x_000 + data_x_250 + data_x_350 # for training, raw images\n",
    "data_y = data_y_000 + data_y_250 + data_y_350 # for training, label images\n",
    "\n",
    "# store paths to validation data (here timepoint 100 in annotated data)\n",
    "data_x_100 = sorted(glob(data_path + \"2022_64x256x256_Platynereis/dataset_hdf5_100/dataset_hdf5_100/images/*.npy\"))\n",
    "data_y_100 = sorted(glob(data_path + \"2022_64x256x256_Platynereis/dataset_hdf5_100/dataset_hdf5_100/masks/*.npy\"))\n",
    "\n",
    "data_val_x = data_x_100\n",
    "data_val_y = data_y_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3363d119-4196-4046-abb3-a0bb35bfa88b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5: Data loaders for intensity images \n",
    "# make changes here if your format is different (than tif or npy)\n",
    "\n",
    "class seq_x(Sequence):\n",
    "\n",
    "    def __init__(self, data_x_trn):\n",
    "        self.data_ = data_x_trn[0:min(num_trn, len(data_x_trn))]\n",
    "        print(\"Total images = {}, Using {}\".format(len(data_x_trn), len(self.data_)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_[idx][-4:] == '.npy':\n",
    "            x = np.load(self.data_[idx])\n",
    "        elif self.data_[idx][-4:] == '.tif':\n",
    "            x = tif.imread(self.data_[idx])\n",
    "        return normalize(x, start_percentile, end_percentile, axis=axis_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d47b0545-e6a3-49c4-863a-a031ab077c40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6: Data loaders for ground-truth label images \n",
    "# make changes here if your format is different\n",
    "\n",
    "class seq_y(Sequence):\n",
    "\n",
    "    def __init__(self, data_y_trn):\n",
    "        self.data_ = data_y_trn[0:min(num_trn, len(data_y_trn))]\n",
    "        print(\"Total images = {}, Using {}\".format(len(data_y_trn), len(self.data_)))\n",
    "        self.ndim = 3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_[idx][-4:] == '.npy':\n",
    "            y = np.load(self.data_[idx])\n",
    "        elif self.data_[idx][-4:] == '.tif':\n",
    "            y = tif.imread(self.data_[idx])\n",
    "        return fill_label_holes(y.astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4b56924-59c7-418f-9a4d-0e3027e9509f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images = 150, Using 150\n",
      "Total images = 150, Using 150\n",
      "Total validation images = 50, ground truth 50\n",
      "Total images = 50, Using 50\n",
      "Total images = 50, Using 50\n",
      "- training:       150\n",
      "- validation:      50\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: constructing the training and validation data\n",
    "# depending on the size, validation is limited to around 20 images when caching\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "ind = rng.permutation(len(data_y))\n",
    "\n",
    "ind_train = ind\n",
    "ind_val = rng.permutation(len(data_val_y))\n",
    "data_x_val, data_y_val = [data_val_x[i] for i in ind_val], [data_val_y[i] for i in ind_val]\n",
    "data_x_trn, data_y_trn = [data_x[i] for i in ind_train], [data_y[i] for i in ind_train]\n",
    "X_trn = seq_x(data_x_trn)\n",
    "Y_trn = seq_y(data_y_trn)\n",
    "print(\"Total validation images = {}, ground truth {}\".format(len(data_val_x), len(data_val_y)))\n",
    "\n",
    "X_val = seq_x(data_x_val)\n",
    "Y_val = seq_y(data_y_val)\n",
    "\n",
    "assert len(X_trn) == len(Y_trn), \"len(X_trn) == len(Y_trn) not satisfied\"\n",
    "assert len(X_val) == len(Y_val), \"len(X_val) == len(Y_val) not satisfied\"\n",
    "\n",
    "print('- training:       %3d' % len(X_trn))\n",
    "print('- validation:     %3d' % len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96647619-39d0-4bc2-90fa-f4ac775f9219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration for a :class:`StarDist3D` model.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    axes : str or None\n",
      "        Axes of the input images.\n",
      "    rays : Rays_Base, int, or None\n",
      "        Ray factory (e.g. Ray_GoldenSpiral).\n",
      "        If an integer then Ray_GoldenSpiral(rays) will be used\n",
      "    n_channel_in : int\n",
      "        Number of channels of given input image (default: 1).\n",
      "    grid : (int,int,int)\n",
      "        Subsampling factors (must be powers of 2) for each of the axes.\n",
      "        Model will predict on a subsampled grid for increased efficiency and larger field of view.\n",
      "    n_classes : None or int\n",
      "        Number of object classes to use for multi-class predection (use None to disable)\n",
      "    anisotropy : (float,float,float)\n",
      "        Anisotropy of objects along each of the axes.\n",
      "        Use ``None`` to disable only for (nearly) isotropic objects shapes.\n",
      "        Also see ``utils.calculate_extents``.\n",
      "    backbone : str\n",
      "        Name of the neural network architecture to be used as backbone.\n",
      "    kwargs : dict\n",
      "        Overwrite (or add) configuration attributes (see below).\n",
      "\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    unet_n_depth : int\n",
      "        Number of U-Net resolution levels (down/up-sampling layers).\n",
      "    unet_kernel_size : (int,int,int)\n",
      "        Convolution kernel size for all (U-Net) convolution layers.\n",
      "    unet_n_filter_base : int\n",
      "        Number of convolution kernels (feature channels) for first U-Net layer.\n",
      "        Doubled after each down-sampling layer.\n",
      "    unet_pool : (int,int,int)\n",
      "        Maxpooling size for all (U-Net) convolution layers.\n",
      "    net_conv_after_unet : int\n",
      "        Number of filters of the extra convolution layer after U-Net (0 to disable).\n",
      "    unet_* : *\n",
      "        Additional parameters for U-net backbone.\n",
      "    resnet_n_blocks : int\n",
      "        Number of ResNet blocks.\n",
      "    resnet_kernel_size : (int,int,int)\n",
      "        Convolution kernel size for all ResNet blocks.\n",
      "    resnet_n_filter_base : int\n",
      "        Number of convolution kernels (feature channels) for ResNet blocks.\n",
      "        (Number is doubled after every downsampling, see ``grid``.)\n",
      "    net_conv_after_resnet : int\n",
      "        Number of filters of the extra convolution layer after ResNet (0 to disable).\n",
      "    resnet_* : *\n",
      "        Additional parameters for ResNet backbone.\n",
      "    train_patch_size : (int,int,int)\n",
      "        Size of patches to be cropped from provided training images.\n",
      "    train_background_reg : float\n",
      "        Regularizer to encourage distance predictions on background regions to be 0.\n",
      "    train_foreground_only : float\n",
      "        Fraction (0..1) of patches that will only be sampled from regions that contain foreground pixels.\n",
      "    train_sample_cache : bool\n",
      "        Activate caching of valid patch regions for all training images (disable to save memory for large datasets)\n",
      "    train_dist_loss : str\n",
      "        Training loss for star-convex polygon distances ('mse' or 'mae').\n",
      "    train_loss_weights : tuple of float\n",
      "        Weights for losses relating to (probability, distance)\n",
      "    train_epochs : int\n",
      "        Number of training epochs.\n",
      "    train_steps_per_epoch : int\n",
      "        Number of parameter update steps per epoch.\n",
      "    train_learning_rate : float\n",
      "        Learning rate for training.\n",
      "    train_batch_size : int\n",
      "        Batch size for training.\n",
      "    train_tensorboard : bool\n",
      "        Enable TensorBoard for monitoring training progress.\n",
      "    train_n_val_patches : int\n",
      "        Number of patches to be extracted from validation images (``None`` = one patch per image).\n",
      "    train_reduce_lr : dict\n",
      "        Parameter :class:`dict` of ReduceLROnPlateau_ callback; set to ``None`` to disable.\n",
      "    use_gpu : bool\n",
      "        Indicate that the data generator should use OpenCL to do computations on the GPU.\n",
      "\n",
      "        .. _ReduceLROnPlateau: https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Cell 8 (OPTIONAL): print out information about parameters in configuration\n",
    "print(Config3D.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d86a41a-c16f-4695-a078-3218da86be53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empirical anisotropy of labeled objects = (8.285714285714286, 1.0, 1.0691244239631337)\n",
      "Config3D(n_dim=3, axes='ZYXC', n_channel_in=1, n_channel_out=97, train_checkpoint='weights_best.h5', train_checkpoint_last='weights_last.h5', train_checkpoint_epoch='weights_now.h5', n_rays=96, grid=(1, 4, 4), anisotropy=(8.285714285714286, 1.0, 1.0691244239631337), backbone='unet', rays_json={'name': 'Rays_GoldenSpiral', 'kwargs': {'n': 96, 'anisotropy': (8.285714285714286, 1.0, 1.0691244239631337)}}, n_classes=None, unet_n_depth=2, unet_kernel_size=(3, 3, 3), unet_n_filter_base=32, unet_n_conv_per_depth=2, unet_pool=(2, 2, 2), unet_activation='relu', unet_last_activation='relu', unet_batch_norm=False, unet_dropout=0.0, unet_prefix='', net_conv_after_unet=128, net_input_shape=(None, None, None, 1), net_mask_shape=(None, None, None, 1), train_patch_size=(32, 256, 256), train_background_reg=0.0001, train_foreground_only=0.9, train_sample_cache=True, train_dist_loss='mae', train_loss_weights=(1, 0.2), train_class_weights=(1, 1), train_epochs=400, train_steps_per_epoch=100, train_learning_rate=0.0003, train_batch_size=2, train_n_val_patches=None, train_tensorboard=True, train_reduce_lr={'factor': 0.5, 'patience': 40, 'min_delta': 0}, use_gpu=True)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: set up the model\n",
    "# and specify anisotropy of images\n",
    "# and specify rays for star-convex shapes\n",
    "Y2 = [Y_trn[i] for i in range(0, len(Y_trn), 100)]\n",
    "extents = calculate_extents(Y2)\n",
    "anisotropy = tuple(np.max(extents) / extents)\n",
    "\n",
    "print('empirical anisotropy of labeled objects = %s' % str(anisotropy))\n",
    "n_rays = 96\n",
    "use_gpu = gputools_available() #True #() # setting this to True did not work (No module named gputools?)\n",
    "grid = tuple(1 if a > 1.5 else 2 for a in anisotropy)  # WTH is this?\n",
    "grid = (1,4,4)\n",
    "rays = Rays_GoldenSpiral(n_rays, anisotropy=anisotropy)\n",
    "\n",
    "conf = Config3D(\n",
    "    rays=rays,\n",
    "    grid=grid,\n",
    "    anisotropy=anisotropy,\n",
    "    use_gpu=use_gpu,\n",
    "    n_channel_in=n_channel,\n",
    "    # adjust for your data below (make patch size as large as possible)\n",
    "    train_patch_size=(z_ptch_size, ptch_size, ptch_size),\n",
    "    # reduce batch size if run out of memory\n",
    "    train_batch_size=2  #\n",
    "    #train_sample_cache = False # LB could try larger batch size (not for validation I think)\n",
    "    #train_learning_rate = .00003\n",
    ")\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f19e201c-2cce-4d64-8ff5-6a33f7d65a25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading thresholds from 'thresholds.json'.\n",
      "Using default values: prob_thresh=0.531428, nms_thresh=0.3.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "median object size:      [ 7.   58.   54.25]\n",
      "network field of view :  [26 93 93]\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: set up the model\n",
    "# Note: this path is to the model you would like to train further.\n",
    "# This training will update the weights stored in that folder -- files will be overwritten.\n",
    "\n",
    "# The \"python3 download_data_and_models_for_finetune.py\" in the setup created a copy of the model to finetune.\n",
    "\n",
    "model_path = path_to_code + \"models\"\n",
    "fldr_name = 'late_blastocyst_model'\n",
    "\n",
    "model = StarDist3D(conf, name=fldr_name, basedir=model_path)\n",
    "median_size = calculate_extents(Y2, np.median)\n",
    "fov = np.array(model._axes_tile_overlap('ZYX'))\n",
    "print(f\"median object size:      {median_size}\")\n",
    "print(f\"network field of view :  {fov}\")\n",
    "if any(median_size > fov):\n",
    "    print(\"WARNING: median object size larger than field of view of the neural network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b8bfbeb-c763-46a5-9559-20503e41eeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 11 (OPTIONAL): modify for your data - augment your data with realistic transformations\n",
    "# TODO: update aug\n",
    "class aug():\n",
    "    def __init__(self):\n",
    "        aug = ImageSegmentationAugmenter()\n",
    "        #aug.add_augmentation(GridWarp(grid=(2,2,1), max_shift=4))\n",
    "        aug.add_augmentation(Flip(0))\n",
    "        aug.add_augmentation(Flip(1))\n",
    "        aug.add_augmentation(Flip(2))\n",
    "        aug.add_augmentation(Identity())\n",
    "        #         aug.add_augmentation(Random_intensity())\n",
    "        self.aug = aug\n",
    "\n",
    "    def __call__(self, img, seg):\n",
    "        img_ = np.expand_dims(img, axis=-1)\n",
    "        seg_ = np.expand_dims(seg, axis=-1).astype(np.float32)\n",
    "        aug_img, aug_seg = self.aug([img_, seg_])\n",
    "        return aug_img[:, :, :, 0].numpy(), aug_seg[:, :, :, 0].numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad128e-f24c-4695-b51c-605aa5c897db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: train and optimize thresholds\n",
    "# Note: This may output some warnings.\n",
    "\n",
    "augumenter_ = None #aug() # currently set augment to none\n",
    "model.train(X_trn, Y_trn, validation_data=(X_val, Y_val), augmenter=augumenter_, \\\n",
    "            epochs=nephochs, steps_per_epoch=nsteps_per_epoch)\n",
    "model.optimize_thresholds(X_val, Y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
